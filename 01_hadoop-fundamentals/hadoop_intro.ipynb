{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d45406b",
   "metadata": {},
   "source": [
    "# Proyecto 01 – Fundamentos de Hadoop\n",
    "\n",
    "Este cuaderno explora los conceptos básicos de Hadoop, especialmente el funcionamiento de MapReduce, usando una simulación en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba4b68",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Hadoop?\n",
    "Hadoop es un framework open-source que permite el procesamiento distribuido de grandes volúmenes de datos a través de clústeres de computadoras usando modelos de programación simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac6138",
   "metadata": {},
   "source": [
    "### Componentes clave de Hadoop:\n",
    "- **HDFS (Hadoop Distributed File System)**: sistema de archivos distribuido.\n",
    "- **MapReduce**: modelo de programación para procesamiento distribuido.\n",
    "- **YARN**: gestor de recursos que coordina las tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0d8a5",
   "metadata": {},
   "source": [
    "## 2. Simulación de MapReduce en Python\n",
    "\n",
    "A continuación, simulamos un proceso típico de MapReduce: contar la frecuencia de palabras en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Simulación del 'Map'\n",
    "def mapper(text):\n",
    "    words = text.split()\n",
    "    mapped = [(word.lower(), 1) for word in words]\n",
    "    return mapped\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"Big data es una tecnología poderosa. Big data cambia el mundo.\"\n",
    "map_output = mapper(texto)\n",
    "print(\"Salida del mapper:\")\n",
    "print(map_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Simulación del 'Shuffle & Sort'\n",
    "from collections import defaultdict\n",
    "\n",
    "def shuffle_and_sort(mapped):\n",
    "    grouped = defaultdict(list)\n",
    "    for word, count in mapped:\n",
    "        grouped[word].append(count)\n",
    "    return grouped\n",
    "\n",
    "grouped_output = shuffle_and_sort(map_output)\n",
    "print(\"\\nAgrupación:\")\n",
    "for word, counts in grouped_output.items():\n",
    "    print(f\"{word}: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Simulación del 'Reduce'\n",
    "def reducer(grouped):\n",
    "    reduced = {word: sum(counts) for word, counts in grouped.items()}\n",
    "    return reduced\n",
    "\n",
    "reduced_output = reducer(grouped_output)\n",
    "print(\"\\nConteo final de palabras:\")\n",
    "print(reduced_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b8372",
   "metadata": {},
   "source": [
    "## 3. Conclusión y reflexión\n",
    "Este ejercicio simula el flujo básico de MapReduce. Aunque Hadoop opera a gran escala y en múltiples nodos, este ejemplo permite comprender cómo se estructura internamente su lógica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9ce36",
   "metadata": {},
   "source": [
    "✍️ **Tu reflexión:**\n",
    "Escribe aquí qué has aprendido sobre MapReduce y qué dudas te quedan."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
